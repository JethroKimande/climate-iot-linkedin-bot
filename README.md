# ğŸŒ Climate-IoT LinkedIn Automation Bot

A Python-based automation system that fetches satellite air pollution data, processes it into insights, visualizes trends, and posts weekly updates to LinkedInâ€”all powered by open data.
ğŸ“¡ Powered by APIs like Caeli and NASA GIBS ğŸ“Š Visualizes pollution metrics via Python charts ğŸ“¢ Auto-publishes commentary through LinkedIn UGC API ğŸ§  Designed for collaboration, impact, and scalability
## ğŸš€ Overview

This project automates the pipeline between open satellite APIs and professional social media. It analyzes pollutants like NOâ‚‚, shares the story through charts and captions, and posts to LinkedIn using scheduled routines. Perfect for climate advocacy, data storytelling, and building your public voice.

## ğŸ“ Folder Structure

<<<<<<< HEAD
```
.
â”œâ”€â”€ apscheduler_api/  # FastAPI server for running jobs
â”‚   â””â”€â”€ main.py
â”œâ”€â”€ scripts/          # Standâ€‘alone job scripts
â”‚   â”œâ”€â”€ fetch_satellite_data.py
â”‚   â”œâ”€â”€ generate_chart.py
â”‚   â”œâ”€â”€ list_gibs_layers.py
â”‚   â””â”€â”€ ptest.py
â”œâ”€â”€ data/             # Output CSVs and images
â”œâ”€â”€ job_logs.txt      # Logs from scheduled runs
â”œâ”€â”€ requirements.txt  # Python dependencies
â””â”€â”€ README.md
```

=======
>>>>>>> main

## ğŸ›°ï¸ Data Sources

- [Caeli API](https://caeli.nl/en/api/) â€“ satellite-based NOâ‚‚, CO, Oâ‚ƒ, SOâ‚‚ data
- [NASA Earthdata (GIBS)](https://earthdata.nasa.gov/) â€“ global air quality maps
- Others optional: OpenWeather Pollution API, Sentinel Hub

## ğŸ§° Dependencies

This project requires **Python&nbsp;3.9+**. Install all packages with:

```bash
pip install -r requirements.txt
```

## ğŸ” Environment Variables

Create a `.env` file in the repository root containing:

```
LINKEDIN_ACCESS_TOKEN=your_token_here
CAELI_API_KEY=your_api_key_here
```

These variables are read by the job scripts at runtime.


â° Automation Schedule
Posts run every Monday at 09:00 AM. Adjust this in `scheduler.py` as needed.

## ğŸš¦ Running the API

Start the FastAPI server with:

```bash
uvicorn apscheduler_api.main:app --reload
```

Trigger an individual job (for example `fetch_satellite_data`) via HTTP:

```bash
curl -X POST http://127.0.0.1:8000/run-job/fetch_satellite_data
```

## ğŸ§ª Running Tests

Install the requirements first, then run `pytest` from the project root:

```bash
pytest
```

ğŸ“¢ Sample LinkedIn Output
ğŸŒ«ï¸ This weekâ€™s NOâ‚‚ average over Nairobi: 18.6 Âµg/mÂ³. Satellite data confirms a 12% drop compared to last week. Could rainfall be clearing the air? Let's talk about climate resilience. #ClimateTech #DataForGood #IoTAfrica

ğŸ¤ Contributing
Open to collaboration:

Add support for PM2.5, Oâ‚ƒ, CO metrics

Map data using GeoJSON or satellite overlays

Translate insights into infographics

Expand automation to multiple regions

ğŸ“œ License
MIT Â© 2025 Jethro

---

## Running the Job API

This repository includes a small FastAPI app that runs jobs defined in the
`scripts/` folder. Install the dependencies and launch it with:

```bash
uvicorn apscheduler_api.main:app --reload
```

Once running, trigger a job by sending a POST request to `/run-job/{job_name}`.
See [docs/api_usage.md](docs/api_usage.md) for a full list of jobs and example
commands.

If youâ€™d like a visual badge setup, GitHub Actions CI trigger, or even a contributorâ€™s guide section, I can snap those in next. Want to keep evolving this into a full-blown dashboard or newsletter tool? Iâ€™m ready when you are! ğŸ˜ğŸ“ˆ

